---
title: "Homework 6"
author: 'Steven Winter, Zining Ma, Qianyin Lu, Mengxuan Cui'
date: "11/14/2019"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      warning = FALSE)
```

## Preliminaries

We need the following packages:
```{r packages}
library(tidyverse)
library(vroom)
#devtools::install_github("tidyverse/multidplyr")
# make sure all other packages are up to date first
library(multidplyr)
library(lubridate)
library(data.table)
library(profvis)
```

## Task 1

We're going to use the code from the class to load the data in parallel. Here we create a cluster with 4 nodes and tell ``R`` to split up loading these 5 files over the four nodes. Ideally this would be done over five nodes, but we were not allowed to use more than four.
```{r loaddata}
base_url = "http://www2.stat.duke.edu/~sms185/data/bike/"
files = c("cbs_2013.csv", "cbs_2014.csv", "cbs_2015.csv", "cbs_2016.csv", "cbs_2017.csv")

cbs_names = c("duration", "start_date", "end_date", "start_station_number", 
               "start_station","start_station_number", "start_station", 
               "bike_number","member_type")

# code from slides
clust <- multidplyr::new_cluster(4)
multidplyr::cluster_assign_partition(clust, file_name = paste0(base_url, files))
multidplyr::cluster_send(clust, cbs_data <- vroom::vroom(file_name))
cbs <- multidplyr::party_df(clust, "cbs_data")
```

We will now use ``rename()`` to replace the current column names with the names from ``cbs_test.csv``. This will make it easier to filter the data later on.
```{r rename}
cbs = cbs %>%
  dplyr::rename(duration = Duration,
         start_date = `Start date`,
         end_date = `End date`,
         start_station_number = `Start station number`,
         start_station = `Start station`,
         end_station_number = `End station number`,
         end_station = `End station`,
         member_type = `Member type`,
         bike_number = `Bike number`)
```

We'll also leverage the cluster to create columns for year, month, day, and hour. These will be used for filtering later.
```{r addcols}
cbs = cbs %>% mutate(year = lubridate::year(start_date),
                     month = lubridate::month(start_date),
                     day = lubridate::day(start_date),
                     hour = lubridate::hour(start_date),
                     # wday returns 2~6 for Monday~Friday
                     weekday = lubridate::wday(start_date) %in% 2:6)
```

That is all of the preprocessing we need to do. Now we will move the data back to our machine with ``collect()`` and convert it to a ``data.table``. The main advantage of data tables is they allow the user to set binary search keys, which makes filtering the data dramatically faster than filtering a data frame. 
```{r collect}
cbs_dt = data.table(collect(cbs))
```

We have to add the weather data before we set any search keys. API limitations forced us to gather all of the weather data in advance - that code is in the `get` folder. The `get_wet.R` script in that folder contains this code:
```{r getweather, eval=FALSE}
key<-"9e3c829d6ef2609428893d0252751748"

st<-as.Date("2013-1-1")
len<-as.Date("2017-12-31")-st

dates<-sapply(0:len, function(i){
  (st+i) %>% as.character()
})

if(!dir.exists("data")){
  dir.create("data")
}

if(!dir.exists("data/weather")){
  dir.create("data/weather")
}

# change iteration range here to get required data
for(i in 1000:1826){
  qr<-paste0("https://api.darksky.net/forecast/9e3c829d6ef2609428893d0252751748/38.9070412,-77.0376289,",dates[i],"T00:00:00?units=si&exclude=minutely")
  fname<-paste0("data/weather/",dates[i],".json")
  #download.file(qr,fname)
}
```

The iteration range can be changed to grab the weather in batches. The `parse_wet.R` script was run after each run of `get_wet.R`; all it does is add the extracted information onto the end of `weather.Rds`. We did not upload the individual weather files because of size limitations.

We don't believe that cyclists care if the weather is clear or partly cloudy; nor do we think there's much of a difference between their behaviour in heavy rain or heavy snow. This leads us to create coarsely binned categories for weather conditions. This new column is added alongside temperature to the data table created above based on time.
```{r addweather}
weather = readRDS("weather.rds")

weat_cond = weather %>% 
  select(year,month,day,hour,weekday,icon,temperature) %>% 
  #weekday 1~5 are Monday~Friday
  mutate(weekday = weekday %in% 1:5,
         icon = recode(icon,
                       `clear-day` = "nice",
                       `clear-night` = "nice",
                       `cloudy` = "nice",
                       `fog` = "bad",
                       `partly-cloudy-day` = "nice",
                       `partly-cloudy-night` = "nice",
                       `rain` = "terrible",
                       `sleet` = "terrible",
                       `snow` = "terrible"))

cbs_dt = merge(cbs_dt,weat_cond,all.x=T,sort = F)
```

We can finally set keys:
```{r keys}
setkey(cbs_dt, start_station, duration, weekday)
```

As mentioned before, this will make searching by ``start_station``, `duration`, or `weekday` very fast.

## Task 2:

Our goal is to beat uniform predictions. To us, the most immediate flaw with uniform predictions is they assign a nonzero probability to stops the cyclist could not have possibly reached in a given duration. Given a test row it seems reasonable to filter out data table to a set of rows with the same `start_station` and similar durations. We can then assign the empirical probability to the end stops in this subset of the data: for example, if there are 30 unique end stations and `A` appears $10$ times then we would set $P(\text{end_station}=A|\text{data})=10/30$. Any end stations not in this subset will be assigned a probability of zero. 

### Helper Functions

Obviously, this method requires us to quickly generate a data table with similar rows (exactly what defines a "similar" row will be determined during the model testing later). The next helper function is the workhorse of our project:
```{r similarrows}
get_similar_rows <- function(row, input_dt, duration_minus = 0.2, duration_plus=0.2,
                             month_delta = Inf, hour_delta=2, temp_delta = Inf,
                             same_weather = FALSE) {
  # filter the data.table to rows with:
  # - the same start station and day of the week
  # - durations in [1-duration_minus, 1+duration_plus]*actual_duration
  # - months/hours/temp within +- respective deltas
  # - same weather category if same_weather
  
  same_start = cbs_dt[J(row$start_station), nomatch=0L]
  
  filtered = same_start[duration %between% c((1-duration_minus)*row$duration, (1+duration_plus)*row$duration) &
            abs(hour-row$hour) <= hour_delta & 
            weekday == row$weekday &
            abs(temperature - row$temperature) <= temp_delta &
            abs(month - row$month)<= month_delta]
  
  if(same_weather){
    return(filtered[icon==row$icon])
  }
  
  return(filtered)
}
```

The next step is to assign the empirical probabilitiy to each row as explained above. 
```{r calculateprobs}
make_predictions <- function(similar_rows, all_stops) {
  # takes in a list of similar rows
  # assigns each unique stop the observed probability
  # assigns any missing stops zero probability
  
  nonzero_probs = as.data.frame(table(similar_rows$end_station)/nrow(similar_rows))
  predictions = spread(nonzero_probs, key=Var1, value=Freq)
  
  zero_probs = all_stops[!(all_stops %in% names(predictions))]
  predictions[zero_probs] = 0
  
  predictions
}
```

There is one small problem here: some test observations are so unlike anything that we have ever seen before that they will have zero similar rows. We can't assign zero probability to every possible end stop, so we'll just stick with the uniform probability. We also need the predictions to be returned in the same order so that we can bind the results into a data table in a reasonable time. The wrapper function below handles these problems.

```{r wrapper}
prediction_wrapper = function(row, input_dt, features, all_stops, ordered_names) {
  # takes in a row from the test_set
  # computes and appends predictions
  
  similar_rows = get_similar_rows(row, input_dt)
  
  if(nrow(similar_rows)==0){
    # if there are are no similar rows in our data, just guess
    return(row[,ordered_names])
  }
  
  # compute and append predictions
  predictions = make_predictions(similar_rows, all_stops)
  output_row = cbind(row[features], predictions)
  
  # return everything in the same order to make rbind faster
  output_row[,ordered_names]
}
```


### Making Predictions

The test data must be loaded before we can make any predictions. Weather is also added at this stage in the same manner as before.
```{r loadtest}
test_set = vroom::vroom("http://www2.stat.duke.edu/~sms185/data/bike/cbs_test.csv")
test_wet = test_set %>% mutate(year = lubridate::year(start_date),
                               month = lubridate::month(start_date),
                               day = lubridate::day(start_date),
                               hour = lubridate::hour(start_date),
                               # wday returns 2~6 for Monday~Friday
                               weekday = lubridate::wday(start_date) %in% 2:6)
test_wet = merge(test_wet,weat_cond,all.x = T,sort = F) %>% arrange(start_date)
```

Here we extract the test data features we can use to fulter rows, as well as all of the possible end stops. 
```{r names}
ordered_names = colnames(test_set)

exclude_features = c("year","month","day","hour","weekday","icon","temperature")
features = c("start_date","end_date","duration","start_station_number",
                  "start_station","bike_number", "member_type")
all_stops = ordered_names[!(ordered_names %in% features)]
```

Producing predictions is as simple as applying our function to every row (`adply` returns a data frame and seems faster than apply). We then drop the extra features we've created and save the results.

```{r predictall}
final_predictions = plyr::adply(test_wet, 1, function(x) prediction_wrapper(x, cbs_dt, features, all_stops, ordered_names))
final_predictions = final_predictions %>% select(-exclude_features)

write_csv(final_predictions, "cbs_git-r-done.csv")
```


### Model selection

Let $x$ be a single test row. Our first model defined a similar row as any row with all of the following conditions:

* the same start station as $x$,
* a start month within $\pm$ 2 months of the start month of $x$,
* a start hour within $\pm$ 2 hours of the start hour of $x$, and
* a duration at least $80\%$ the duration of $x$ and not more than $120\%$.

This model gave us a score of 5.61, but did not incorporate weather. Our second model had the same settings as above, but restricted to rows with:

* the same type of weather (nice, bad, terrible) as $x$, 
* a temperature within $10$ degrees of $x$, and
* the same `weekday` indicator as $x$.

This model was terrible: we got a score around 3.75. We tried adding the following combinations of varaibles to the above:

* Same weekday indicator and temperature within seven degrees. Got a score of 5.23.
* Same weekday indicator and same weather conditions. Got a score of 5.43.
* Same weekday indicator. Got a score of 5.77.

Shockingly, weekday only gave us the best score yet. We believe that the weather variables are too noisy to provide useful information with a crude filtering technique - as such, weather was not included in any model after this. We also tried incredibly simple models, such as a model with only weekday. This resulted in a miserable score of 1.97, which indicates that duration and time are still relevant in some sense.

Changing the duration range always resulted in a worse score, and dropping month resulted in a slight improvement. Ultimately we settled on the model where a similar row has

* the same start station as $x$,
* the same weekday indicator as $x$, 
* a start hour within $\pm$ 2 hours of the start hour of $x$, and
* a duration at least $80\%$ the duration of $x$ and not more than $120\%$.

This gave us a score of 6.15, which we have been unable to top.

### Model Analysis

Our model is purely deterministic and consequently has no variance between repeated runs on the same test data. Our filtering criterion are not at all strict: it is unlikely we are over fitting anything. We are proud of the relatively impressive performance given the simplicity of our model.

However, there are two potential drawbacks. First, our filtering criterion throws out a lot of data that might have useful information. Second, our filtering criterion never accounts for outliers - this means that we will almost always fail to predict these edge test cases.

### Profiling

Profiling the code all at once resulted in a massive html file that was too big to upload to github, so we've opted to profile the first half and the last half of the code seperately. We simply wrapped everything from the Task 1 in `profvis` to produce `preliminary_profile.html` and wrapped everything from Task 2 with the same function to produce `prediction_profile.html`. 

In `prediction_profile.html` we see that loading the data takes an exorbitant amount of time (between 6-7 min). This could be reduced by using 5 cores instead of 4, so we would be able to load one file per core instead of having one core have to do twice the work of the others. Collecting the data takes another minute; not much we could do to speed this up. Merging the weather data is suprisingly fast, only taking around 20 seconds.

The time it takes to make predictions is highly variable depending on how well the server is behaving. We see in `prediction_profile.html` that the most time consuming step at each prediction is filtering the data, as expected. It appears that data.tables are the fastest way to filter these sorts of data structures in `R`, so it is unlikely we could make much of an improvement here. We could speed things up considerably by using the parallelzied version of `adply`, but unfortunately we weren't able to get that working in the alloted time. Lastly, we see that `rbinding` the data together after making each prediction took up $20\%$ of the total prediction time. There's probably a fancy function that does this faster.











